# -*- coding:utf-8 -*-from urllib.parse import urlparse__author__ = 'xhp'from celery import Celeryfrom celery import Taskfrom celery.utils.log import get_task_loggerfrom dns import resolverfrom hashlib import md5from tld import get_tldimport time, os, sysfrom math import ceil, floorfrom PIL import Imagefrom threading import Timerimport subprocessimport jsonimport zipfileimport shutilfrom datetime import datetimefrom dateutil import parserimport logging'''#todo file_hash.csv    struct:    filename,sha256    maybe filepath need to modified.'''logger = get_task_logger(__name__)logging.basicConfig(level = logging.INFO,format='%(asctime)s - %(funcName)s - %(lineno)d - %(levelname)s - %(message)s',stream=sys.stdout)# broker_url = 'amqp://ubuntu:123456@172.16.87.167:5672/ubuntu'# REDIS_HOST = '10.0.5.34'# REDIS_PORT = '6379'# REDIS_DB = 13NORMAL_SCALE = 29.7 / 21 / 4app = Celery('scrapyTask')app.config_from_object('celeryconfig')# print app.conf.humanize(with_defaults=False, censored=True)class DebugTask(Task):    def __call__(self, *args, **kwargs):        # print('TASK STARTING: {0.name}[{0.request.id}]'.format(self))        return super(DebugTask, self).__call__(*args, **kwargs)app.Task = DebugTaskdef zip_ya(startdir, file_news,remove_target=False):    logging.info(u'开始打包路径：%s, 请稍候...' % startdir)    z = zipfile.ZipFile(file_news, 'w', zipfile.ZIP_DEFLATED)  # 参数一：文件夹名    parent_dir = os.sep.join(startdir.split(os.sep)[:-1])    for dirpath, dirnames, filenames in os.walk(startdir):        fpath = dirpath.replace(parent_dir, '')  # 这一句很重要，不replace的话，就从根目录开始复制        fpath = fpath and fpath + os.sep or ''  # 这句话理解我也点郁闷，实现当前文件夹以及包含的所有文件的压缩        for filename in filenames:            z.write(os.path.join(dirpath, filename), fpath + filename)    z.close()    logging.info(u'压缩成功')    if remove_target:        try:            shutil.rmtree(startdir,True)            logging.info('压缩源文件已删除.')        except Exception as e:            logging.error(e.message)def file_hash(startdir):    logging.info(u'开始处理文件SHA256：%s, 请稍候...' % startdir)    import hashlib, csv    hashfile = open(os.path.join(startdir, 'file_hash.csv'), 'w')    writer = csv.writer(hashfile)    for dirpath, dirnames, filenames in os.walk(startdir):        for f in filenames:            tmp = open(os.path.join(dirpath, f), 'rb')            sha256 = hashlib.sha256()            sha256.update(tmp.read())            writer.writerow((os.path.join(dirpath, f), sha256.hexdigest()))            tmp.close()    hashfile.close()@app.task(base=DebugTask, queue='websnap')def scrapyNormal(url, config):    '''     return datastruct {'log':[],'result':{}}     result:     {u'sceenshot_file': u'./output/fac217fa85f31123ce1c70aae99d986f/screenshot.png',     u'data_path': u'./output/fac217fa85f31123ce1c70aae99d986f',     u'config:{},     u'whois': {u'status': [u'clientDeleteProhibited https://icann.org/epp#clientDeleteProhibited', u'clientTransferProhibited https://icann.org/epp#clientTransferProhibited', u'clientUpdateProhibited https://icann.org/epp#clientUpdateProhibited', u'serverDeleteProhibited https://icann.org/epp#serverDeleteProhibited', u'serverTransferProhibited https://icann.org/epp#serverTransferProhibited', u'serverUpdateProhibited https://icann.org/epp#serverUpdateProhibited'], u'updated_date': [1501180588000], u'domain': u'baidu.com', u'creation_date': [939611117000], u'whois_server': [u'whois.markmonitor.com'], u'emails': [u'abusecomplaints@markmonitor.com'], u'billing': None, u'admin': None, u'expiration_date': [1791687917000], u'registrant': None, u'nameservers': [u'DNS.BAIDU.COM', u'NS2.BAIDU.COM', u'NS3.BAIDU.COM', u'NS4.BAIDU.COM', u'NS7.BAIDU.COM'], u'registrar': [u'MarkMonitor Inc.'], u'tech': None},     u'dns': {u'nameservers': [u'202.96.209.133'], u'ips': [u'115.239.211.112', u'115.239.210.27'],     u'domain': u'www.baidu.com'}}     config:     {"request_timeout": 300, "cookies": null, "exts": [], "timeout": 60000, "pre_run": false, "width": 1280,     "request_interval_timeout": 1000, "charset": "utf8", "height": 1024, "headers": null, "inject_js": null,     "js_enable": true,     "user_agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36",     "screenshot_thumbnail_format": "png", "inject_css": null, "wait_before_screenshot": 1000,     "output_path": "./output", "url_filters": [], "screenshot_thumbnail_filename": "screenshot"}    :param url:    :param config:    :return:    '''    # todo 任务失败时处理    # todo 任务完成后删除临时文件    result = {"ok": True, "url": url,"result":{}}    proc = subprocess.Popen(        args=["node", "./scrapy/scrapy.js", "-c", json.dumps(config),              "-u", url],        stdin=subprocess.PIPE, stdout=subprocess.PIPE,        stderr=subprocess.PIPE, close_fds=True    )    node_out, node_err = proc.communicate()    node_out = str(node_out, encoding="utf8")    node_err = str(node_err, encoding="utf8")    if proc.returncode == 0:        logger.info("Node return out: {}".format(node_out))        logger.info("Node return err: {}".format(node_err))        logging.info("抓取结果：" + node_out.strip())        result["result"].update(json.loads(node_out.strip()))    else:        logger.error("node生成出错: {}".format(node_err))        result['ok']=False        raise Exception(node_err)    logger.info("out subprocess")    logger.info(json.dumps(result))    # result = json.loads(result)    result['result']['url'] = url    if result["result"].__contains__('log_file'):        logfile = open(result["result"]['log_file'],'a+')    else:        logfile = open(os.path.join(result["result"]['data_path'], 'tmp.log'), 'a+')        logfile.flush()        result["result"]['log_file']=os.path.join(result["result"]['data_path'], 'tmp.log')    try:        result['result']['dns'] = get_dns(url)        logfile.write('{}:{}\n'.format(datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S'),"DNS获取成功."))        logfile.flush()    except Exception as e:        logging.info(e)        result['result']['dns']={'error':e}        logging.info('忽略DNS...')    try:        result['result']['whois'] = get_whois(url)        logfile.write('{}:{}\n'.format(datetime.strftime(datetime.now(), '%Y-%m-%d %H:%M:%S'), "Whois获取成功."))        logfile.flush()    except Exception as e:        logging.info(e)        # result['result']['whois']={'error':str(e)}        logging.info('忽略Whois...')    if 'sceenshot_files' in result['result'] and len(result['result']['sceenshot_files']) > 0:        result['result']['sceenshot_slice'] = result['result']['sceenshot_files']    elif 'sceenshot_file' in result['result'] and result['result']['sceenshot_file'] != '':        result['result']['sceenshot_slice'] = cut_image(result['result']['sceenshot_file'])    result['result']['thumbnail'] = result['result']['thumbnail_file']    logfile.write('{}:{}\n'.format(datetime.strftime(datetime.now(),  '%Y-%m-%d %H:%M:%S'), "任务完成！"))    logfile.flush()    logfile.close()    json.dump(result, open(os.path.join(result['result']['data_path'], 'result.json'), 'w'))  # 导出所有返回结果    json.dump(result['result']['dns'], open(os.path.join(result['result']['data_path'], 'dns.json'), 'w'))  # 导出dns    json.dump(result['result']['whois'],              open(os.path.join(result['result']['data_path'], 'whois.json'), 'w'))  # 导出whois    startdir = os.path.join(result['result']['data_path'], result['result']['tags'][0])  # 要压缩的文件夹路径    file_news = os.path.join(result['result']['data_path'], 'source.zip')  # 压缩后文件夹的名字    zip_ya(startdir, file_news,True)  # 打包目录    file_hash(result['result']['data_path'])  # 编列计算hash    logging.info('任务完成！')    logger.info(result)    return resultdef get_md5(url):    url = url.encode("utf-8")    m = md5()    m.update(url)    return m.hexdigest()def get_dns(url):    logging.info('开始处理DNS, 请稍候...')    domain = urlparse(url).hostname    ips = []    for ip in resolver.query(domain):        ips.append(ip.to_text())    a_dns = {'domain': domain, 'ips': ips, 'nameservers': resolver.get_default_resolver().nameservers}    logging.info('DNS完成')    return a_dnsdef process_datetime(datetime_list):    result = []    if type(datetime_list) == list:        for item in datetime_list:            if type(item) == datetime:                result.append(item)            elif type(item) == str:                try:                    result.append(parser.parse(item))                except Exception as e:                    logging.info(e)    elif type(datetime_list) == str:                try:                    result.append(parser.parse(datetime_list))                except Exception as e:                    logging.info(e)    elif type(datetime_list) == datetime:        result.append(datetime_list)    return resultdef get_whois(url):    # todo 存在卡死情况    logging.info('开始处理WHOIS, 请稍候...')    import whois as whoislib    try:        for i in range(5):            try:                raw_whois = whoislib.whois(url)                whois = {"domain": raw_whois.domain, 'status': raw_whois.get('status', [])}                creation_date = process_datetime(raw_whois.get('creation_date', []))                whois['creation_date'] = [int(time.mktime(creation_date[0].timetuple()) * 1000)] if creation_date else []                expiration_date = process_datetime(raw_whois.get('expiration_date', []))                whois['expiration_date'] = [int(time.mktime(expiration_date[0].timetuple()) * 1000)] if expiration_date else []                updated_date = process_datetime(raw_whois.get('updated_date', []))                whois['updated_date'] = [int(time.mktime(updated_date[0].timetuple()) * 1000)] if updated_date else []                whois['registrar'] = [raw_whois.get('registrar')]                whois['whois_server'] = [raw_whois.get('whois_server')]                whois['nameservers'] = [raw_whois.get('nameservers')]                whois['emails'] = [raw_whois.get('emails')]                whois['raw'] = raw_whois.text                #construct registrant                registrant = None                if raw_whois.get('org'):                    registrant ={}                    registrant['organization'] = raw_whois.get('org','')                    registrant['name'] = raw_whois.get('name','')                    registrant['country'] = raw_whois.get('country','')                    registrant['city'] = raw_whois.get('city','')                    registrant['state'] = raw_whois.get('state',' ')                    registrant['street'] = raw_whois.get('address','')                    registrant['email'] = raw_whois.get('emails','')                    registrant['phone'] = raw_whois.get('phone','')                    registrant['fax'] = raw_whois.get('fax','')                whois['registrant'] = registrant                whois['tech'] = raw_whois.get('contacts', {}).get('tech', None)                whois['admin'] = raw_whois.get('contacts', {}).get('admin', None)                whois['billing'] = raw_whois.get('contacts', {}).get('billing', None)                logging.info('WHOIS完成')                return whois            except Exception as e:                logging.info(e)                logging.info('第%s次WHOIS获取失败，5s后重试'%(i+1))                time.sleep(5)        raise Exception('WHOIS获取失败.')    except Exception as e:        raise edef get_whois2(url):    # todo 存在卡死情况    print('开始处理WHOIS, 请稍候...')    import pythonwhois    try:        for i in range(5):            try:                domain = get_tld(url)                raw_whois = pythonwhois.get_whois(domain)                whois = {"domain": domain, 'status': raw_whois.get('status', [])}                creation_date = list(set(raw_whois.get('creation_date', [])))                whois['creation_date'] = [int(time.mktime(creation_date[0].timetuple()) * 1000)] if creation_date else []                expiration_date = list(set(raw_whois.get('expiration_date', [])))                whois['expiration_date'] = [int(time.mktime(expiration_date[0].timetuple()) * 1000)] if expiration_date else []                updated_date = list(set(raw_whois.get('updated_date', [])))                whois['updated_date'] = [int(time.mktime(updated_date[0].timetuple()) * 1000)] if updated_date else []                whois['registrar'] = list(set(raw_whois.get('registrar', [])))                whois['whois_server'] = list(set(raw_whois.get('whois_server', [])))                whois['nameservers'] = raw_whois.get('nameservers', [])                whois['emails'] = list(set(raw_whois.get('emails', [])))                whois['registrant'] = raw_whois.get('contacts', {}).get('registrant', None)                whois['tech'] = raw_whois.get('contacts', {}).get('tech', None)                whois['admin'] = raw_whois.get('contacts', {}).get('admin', None)                whois['billing'] = raw_whois.get('contacts', {}).get('billing', None)                logging.info('WHOIS完成')                return whois            except Exception as e:                logging.info(e)                logging.info('第%s次WHOIS获取失败，5s后重试'%i+1)                time.sleep(5)        raise Exception('WHOIS获取失败.')    except Exception as e:        logging.info(e)        raise edef cut_image(filename, scale=29.7 / 21 / 4):    """    宽度保持不变，按高宽比例切割网络图片，并保存至azure的blob存储    :param url: 图片的网址    :param scale: 高宽的比率  height / width    :param storage_path: azure上存储的路径    :return: list 分隔号图片的url    """    print('开始处理图片切割, 请稍候...')    # THUMBNAIL_SCALE = 8 / 12.8    s = os.path.basename(filename)    (shotname, extension) = os.path.splitext(s)    img = Image.open(filename)    w, h = img.size    return_img_slice = []    if float(h - 0) / w > scale:        logging.info('原始图片信息: %sx%s, %s, %s' % (w, h, img.format, img.mode))        num = 0        rowheight = floor(w * scale)        slices = int(ceil((h - 0) / rowheight))        return_img_slice = []        for r in range(slices):            height = h if r == (slices - 1) else (r + 1) * rowheight + 0            box = (0, r * rowheight + 0, w, height)  # 加30是为了去除顶部的水印            slice_file = shotname + '_' + str(num) + extension            tmp_file = os.path.join(os.path.dirname(filename), slice_file)            img.crop(box).save(tmp_file, extension[1:])            num = num + 1            return_img_slice.append(tmp_file)        logger.info('图片切割完毕，共生成 %s 张小图片。' % num)        # _t = time.time()        # print "cutting and uploading snapshot spent {} seconds".format(str(_t - start_time))    else:        logging.info('无需分隔，只去顶部水印')        box = (0, 0, w, h)  # 加30是为了去除顶部的水印        slice_file = shotname + '_' + '0' + extension        tmp_file = os.path.join(os.path.dirname(filename), slice_file)        img.crop(box).save(tmp_file, extension[1:])        return_img_slice.append(slice_file)    return return_img_slicedef thumbnail(filename, scale=29.7 / 21 / 4):    """    宽度保持不变，按高宽比例切割网络图片，并保存至azure的blob存储    :param url: 图片的网址    :param scale: 高宽的比率  height / width    :param storage_path: azure上存储的路径    :return: list 分隔号图片的url    """    print('开始处理预览图, 请稍候...')    THUMBNAIL_SCALE = 8 / 12.8    s = os.path.basename(filename)    (shotname, extension) = os.path.splitext(s)    img = Image.open(filename)    w, h = img.size    return_img_slice = []    thumbnail_height = int(floor(w * THUMBNAIL_SCALE)) if (h - 0) > int(floor(w * THUMBNAIL_SCALE)) else h    box = (0, 0, w, thumbnail_height)  # 加30是为了去除顶部的水印    slice_file = 'thumbnail' + extension    tmp_file = os.path.join(os.path.dirname(filename), slice_file)    img.crop(box).save(tmp_file, extension[1:])    return tmp_file